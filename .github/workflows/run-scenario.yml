name: Run Scenario

on:
  workflow_dispatch:  # Allow manual triggering
  push:
    paths:
      - 'scenario.toml'

jobs:
  run:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      packages: read

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install tomli tomli-w

      - name: Generate docker-compose.yml
        run: python generate_compose.py --scenario scenario.toml

      - name: Create output directory
        run: mkdir -p output && chmod 777 output

      - name: Export required secrets to environment file
        env:
          # Only export specific required secrets - never dump all secrets
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        run: |
          # Create .env with only the secrets needed for evaluation
          # This prevents accidental exposure of unrelated secrets
          echo "GOOGLE_API_KEY=${GOOGLE_API_KEY}" > .env
          # Validate the file was created with content (not empty values)
          if [ ! -s .env ]; then
            echo "::error::Failed to create .env file"
            exit 1
          fi

      - name: Validate required secrets
        run: |
          set -e
          # Check that .env file exists and has content
          if [ ! -f .env ]; then
            echo "::error::.env file was not created. Check the previous step."
            exit 1
          fi

          # Validate GOOGLE_API_KEY is present and non-empty
          if ! grep -q "^GOOGLE_API_KEY=.\+" .env; then
            echo "::error::GOOGLE_API_KEY secret is missing or empty."
            echo "::error::Please add GOOGLE_API_KEY to your repository secrets:"
            echo "::error::  1. Go to Settings → Secrets and variables → Actions"
            echo "::error::  2. Click 'New repository secret'"
            echo "::error::  3. Name: GOOGLE_API_KEY, Value: your Gemini API key"
            exit 1
          fi

          # Validate key appears to have reasonable length (API keys are typically 39+ chars)
          KEY_LENGTH=$(grep "^GOOGLE_API_KEY=" .env | cut -d= -f2 | tr -d '\n' | wc -c)
          if [ "$KEY_LENGTH" -lt 20 ]; then
            echo "::error::GOOGLE_API_KEY appears too short (${KEY_LENGTH} chars). Verify the secret value."
            exit 1
          fi

          echo "✓ GOOGLE_API_KEY validated (${KEY_LENGTH} characters)"

      - name: Check if GHCR_TOKEN is available
        id: check_ghcr
        env:
          GHCR_TOKEN: ${{ secrets.GHCR_TOKEN }}
        run: |
          if [ -n "$GHCR_TOKEN" ]; then
            echo "has_token=true" >> $GITHUB_OUTPUT
          else
            echo "has_token=false" >> $GITHUB_OUTPUT
          fi

      - name: Login to GitHub Container Registry
        if: steps.check_ghcr.outputs.has_token == 'true'
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_TOKEN }}

      - name: Run assessment
        run: docker compose --env-file .env up --pull always --exit-code-from agentbeats-client --abort-on-container-exit

      - name: Capture docker logs
        if: always()
        run: |
          docker compose logs > docker-compose.log 2>&1 || true

      - name: Analyze logs for common issues
        if: always()
        run: |
          echo "=== Diagnostic Analysis ==="

          # Check for API key issues
          if grep -qi "api.key\|api_key\|authentication" docker-compose.log 2>/dev/null; then
            echo "::warning::Potential API key issue detected in logs"
            grep -i "api.key\|api_key\|authentication" docker-compose.log | head -5
          fi

          # Check for conversation turns
          TURNS=$(grep -o "Conversation complete ([0-9]* turns)" docker-compose.log 2>/dev/null | head -1 || echo "")
          if [ -n "$TURNS" ]; then
            echo "✓ $TURNS"
          else
            echo "::warning::No conversation completion message found"
          fi

          # Check for classifier errors
          if grep -q "Error classifying response" docker-compose.log 2>/dev/null; then
            echo "::warning::Classifier errors detected - may cause premature conversation end"
            grep "Error classifying response" docker-compose.log | head -3
          fi

          # Check for evaluation completion
          if grep -q "evaluation_complete.*true" docker-compose.log 2>/dev/null; then
            echo "✓ Evaluation completed successfully"
          elif grep -q "evaluation_complete.*false" docker-compose.log 2>/dev/null; then
            echo "::error::Evaluation failed - check logs for details"
          fi

          # Show average score if available
          SCORE=$(grep -o '"average_score":[^,}]*' docker-compose.log 2>/dev/null | head -1 || echo "")
          if [ -n "$SCORE" ]; then
            echo "✓ $SCORE"
          fi

      - name: Upload docker logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: docker-logs
          path: docker-compose.log
          retention-days: 7

      - name: Generate submission metadata
        id: metadata
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          USERNAME=${{ github.repository_owner }}
          UNIQUE_NAME="${USERNAME}-${TIMESTAMP}"
          echo "unique_name=$UNIQUE_NAME" >> $GITHUB_OUTPUT
          echo "branch_name=submission-${UNIQUE_NAME}" >> $GITHUB_OUTPUT

      - name: Copy files to submission directory
        run: |
          set -e
          cp scenario.toml submissions/${{ steps.metadata.outputs.unique_name }}.toml

          # Prefer green-agent's direct output (evaluation_results.json) over agentbeats-client (results.json)
          RESULTS_FILE=""
          if [ -f output/evaluation_results.json ]; then
            echo "✓ Using green-agent's evaluation_results.json"
            RESULTS_FILE="output/evaluation_results.json"
          elif [ -f output/results.json ]; then
            echo "::warning::evaluation_results.json not found, falling back to results.json"
            RESULTS_FILE="output/results.json"
          else
            echo "::error::No results file found. Evaluation may have failed."
            exit 1
          fi

          # Validate results file has actual evaluation data (not a stub)
          if ! grep -q '"evaluation_complete"' "$RESULTS_FILE"; then
            echo "::warning::Results file may be incomplete - missing evaluation_complete field"
            echo "Contents of $RESULTS_FILE:"
            cat "$RESULTS_FILE"
          fi

          # Check for non-empty results array
          RESULTS_COUNT=$(python3 -c "import json; data=json.load(open('$RESULTS_FILE')); print(len(data.get('results', [])))" 2>/dev/null || echo "0")
          if [ "$RESULTS_COUNT" = "0" ]; then
            echo "::error::Results file has empty results array - evaluation may have failed"
            echo "Contents of $RESULTS_FILE:"
            cat "$RESULTS_FILE"
            exit 1
          fi

          echo "✓ Results file contains $RESULTS_COUNT evaluation results"
          cp "$RESULTS_FILE" results/${{ steps.metadata.outputs.unique_name }}.json

      - name: Determine target repository
        id: target
        env:
            GH_TOKEN: ${{ github.token }}
        run: |
            PARENT_REPO=$(gh api repos/${{ github.repository }} --jq '.parent.full_name // "${{ github.repository }}"')
            echo "Target repository: $PARENT_REPO"
            echo "repo=$PARENT_REPO" >> $GITHUB_OUTPUT

      - name: Create submission branch
        run: |
          # Fetch latest from origin (fork) to ensure we have current workflow files
          git fetch origin
          # Create branch from origin/main to avoid workflow file conflicts
          git checkout -b ${{ steps.metadata.outputs.branch_name }} origin/main
          # Also add upstream for the PR link to work
          git remote add upstream https://github.com/${{ steps.target.outputs.repo }}.git || true

      - name: Commit results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add submissions/${{ steps.metadata.outputs.unique_name }}.toml results/${{ steps.metadata.outputs.unique_name }}.json
          git commit -m "Submission: ${{ steps.metadata.outputs.unique_name }}

          Workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          git push origin ${{ steps.metadata.outputs.branch_name }}
          echo "::notice title=Submission Branch Created::Your results are ready for submission on branch: ${{ steps.metadata.outputs.branch_name }}"

      - name: Output PR link
        run: |
          echo "### Submit your results" >> $GITHUB_STEP_SUMMARY
          echo "[Click here to open a pull request](https://github.com/${{ steps.target.outputs.repo }}/compare/main...${{ github.repository_owner }}:${{ github.event.repository.name }}:${{ steps.metadata.outputs.branch_name }}?expand=1)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "⚠️ When creating the PR, UNCHECK 'Allow edits and access to secrets by maintainers' to protect your secrets." >> $GITHUB_STEP_SUMMARY

      - name: Cleanup
        if: always()
        run: docker compose down -v || true
